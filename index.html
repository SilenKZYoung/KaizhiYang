<head>
    <title>Kaizhi Yang</title>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="resources/main.css">
</head>

<body>

<table>
<tr>
<td><img src="images/picture.jpg" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Kaizhi Yang 杨开智</div><br/>
<div>
Master Student<br/>
<a href="http://mcc.ustc.edu.cn" target='_blank'>MCC-Lab</a><br/>
<a href="https://www.ustc.edu.cn" target='_blank'>University Of Science And Technology Of China</a><br/><br/>
</div>
<div>
    <b>Address:</b> University of Science and Technology of China, Hefei, Anhui, China, 230027<br>
    <b>Email:</b> ykz0923@mail.ustc.edu.cn<br>
</div>
<div>
<br>
<a href="https://github.com/SilenKZYoung" target='_blank'>GitHub</a>
</div>
</td>
</tr>
</table>
<br>

<h3>Biography</h3>
<div class="section">
<ul>
    <p>I am a 2nd year Master student at University Of Science And Technology Of China, under the supervision of <a href="http://staff.ustc.edu.cn/~xjchen99/" target="_blank">A.P. Xuejin Chen</a>. </p>
    <p>I received my Bachelor's degree from Nanjing University of Aeronautics and Astronautics in 2019.</p>
    <p>My research interest include 3D vision, graphics and deep learning.</p>

</ul>
</div>
<br>

<h3>News</h3>
<div class="section">
<ul>
    <li> Oct. 2021: Our paper <strong>"Learning Scale-Adaptive Representations for Point-Level LiDAR Semantic Segmentation"</strong> has been accepted by 3DV 2021.
    <li> Apr. 2021: Our paper <strong>"Unsupervised Learning for Cuboid Shape Abstraction via Joint Segmentation from Point Clouds"</strong> has been accepted by ACM SIGGRAPH 2021.
</ul>
</div>
<br>


<a name="publications"></a>
<h3>Publications</h3>
<div class="mainsection">
<ul>

<table border="0"> <tbody>

<!-- ScaleAdaptiveSegmentation -->
<tr>
<td width="400" valign="top"><p><img src="papers/ScaleAdaptiveSegmentation.PNG" width="400" align="top"></p></td>
<td width="1200" valign="top"><p>
    <h3>Learning Scale-Adaptive Representations for Point-Level LiDAR Semantic Segmentation</h3>
    Tongfeng Zhang, 
    <a href="http://staff.ustc.edu.cn/~xjchen99/" target="_blank">Xuejin Chen</a>,
    <strong>Kaizhi Yang</strong><br>
    Accepted to 3DV 2021<br>
    <p>
        The massive objects with various scales and categories in autonomous driving scenes pose a great challenge to the LiDAR semantic segmentation task. Although the voxel-based 3d convolutional networks employed by existing state-of-the-art methods can extract features with different spatial scales, they cannot conduct effective discrimination and combination on them. In this paper, we propose a Scale-Adaptive Fusion (SAF) module that can progressively and selectively fuse features with different receptive fields to help the network deal with scale variations across objects adaptively. In addition, we propose a novel Local Point Refinement (LPR) module to address the quantization loss problem of voxel-based methods. It could take the geometric structure of original point cloud into account by converting voxel-wise feature to the point-wise one. Our proposed method is evaluated on three public datasets, i.e., SemanticKITTI, SemanticPOSS and nuScenes dataset and achieves competitive performance.
    </p>
<!--     <a href="https://arxiv.org/abs/2106.03437" target="_blank">[Paper]</a> -->
<!--     <a href="https://github.com/SilenKZYoung/CuboidAbstractionViaSeg" target="_blank">[Github]</a> -->
</p></td>
</tr>
<tr><td><br/></td></tr>
	
<!-- CuboidAbstractionViaSeg -->
<tr>
<td width="400" valign="top"><p><img src="papers/CuboidAbstractionViaSeg.png" width="400" align="top"></p></td>
<td width="1200" valign="top"><p>
    <h3>Unsupervised Learning for Cuboid Shape Abstraction via Joint Segmentation from Point Clouds</h3>
    <strong>Kaizhi Yang</strong>, 
    <a href="http://staff.ustc.edu.cn/~xjchen99/" target="_blank">Xuejin Chen</a><br>
    Accepted to SIGGRAPH 2021<br>
    <p>
        Representing complex 3D objects as simple geometric primitives, known as shape abstraction, is important for geometric modeling, structural analysis, and shape synthesis. In this paper, we propose an unsupervised shape abstraction method to map a point cloud into a compact cuboid representation. We jointly predict cuboid allocation as part segmentation and cuboid shapes and enforce the consistency between the segmentation and shape abstraction for self-learning. For the cuboid abstraction task, we transform the input point cloud into a set of parametric cuboids using a variational auto-encoder network. The segmentation network allocates each point into a cuboid considering the point-cuboid affinity. In addition, several novel losses are designed to jointly supervise the two branches in terms of geometric similarity and cuboid compactness.
    </p>
    <a href="https://arxiv.org/abs/2106.03437" target="_blank">[Paper]</a>
    <a href="https://github.com/SilenKZYoung/CuboidAbstractionViaSeg" target="_blank">[Github]</a>
</p></td>
</tr>
<tr><td><br/></td></tr>

<!-- BodyFromSketch -->
<tr>
    <td width="400" valign="top"><p><img src="papers/BodyFromSketch.png" width="400" align="top"></p></td>
    <td width="1200" valign="top"><p>
        <h3>Deep 3D Modeling of Human Bodies from Freehand Sketching</h3>
        <strong>Kaizhi Yang</strong>, 
        Jintao Lu,
        <a href="https://samhu1989.github.io/" target="_blank">Siyu Hu</a>,
        <a href="http://staff.ustc.edu.cn/~xjchen99/" target="_blank">Xuejin Chen</a><br>
        Accepted to MMM 2021<br>
        <p>
            Creating high-quality 3D human body models by freehand sketching is challenging because of the sparsity and ambiguity of hand-drawn strokes. In this paper, we present a sketch-based modeling system for human bodies using deep neural networks. Considering the large variety of human body shapes and poses, we adopt the widely-used parametric representation, SMPL, to produce high-quality models that are compatible with many further applications. In order to solve the huge ambiguity in mapping sketches onto the manifold of human bodies, we introduce the skeleton as the intermediate representation. Our skeleton-aware modeling network first interprets sparse joints from coarse sketches and then predicts the SMPL parameters based on joint-wise features. This skeleton-aware intermediate representation effectively reduces the ambiguity and complexity between the two high-dimensional spaces. Based on our light-weight interpretation network, our system supports interactive creation and editing of 3D human body models by freehand sketching.
        </p>
        <a href="https://link.springer.com/chapter/10.1007/978-3-030-67835-7_4" target="_blank">[Paper]</a>
    </p></td>
    </tr>


</tbody> </table>


</table>
</ul>
</div>
<br>

<a name="education"></a>
<h3>Education</h3>
<div class="mainsection">
<ul>
<table>

<!-- USTC -->
<tr>
    <td width="150" valign="top"><p><img src="logos/USTC.jpg" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="1200" valign="top">
        <p><b>Master, University of Science and Technology of China</b><br/>2019.9 - Present<br/><br/>
        Advisor:
            <a href="http://staff.ustc.edu.cn/~xjchen99/" target="_blank">A.P. Xuejin Chen</a>
        </p></td>
</tr>
<tr></tr>

<tr><td><br/></td></tr>

<!-- NUAA -->
<tr>
    <td width="150" valign="top"><p><img src="logos/NUAA.jpg" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="1200" valign="top">
        <p><b>Bachelor, Nanjing University of Aeronautics and Astronautics</b><br/>2015.9 - 2019.6<br/><br/>
        </p></td>
</tr>
<tr></tr>

</table>
</ul>
</div>
<br>

</body>
