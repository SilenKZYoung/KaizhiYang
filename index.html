<head>
    <title>Kaichun Mo</title>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="resources/main.css">
</head>

<body>

<table>
<tr>
<td><img src="images/face.jpg" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Kaichun Mo 莫凯淳</div>
<div>
CS Ph.D. Candidate<br/>
<a href="http://geometry.stanford.edu" target='_blank'>Geometric Computation Group</a> and 
<a href="http://ai.stanford.edu" target='_blank'>Artificial Intelligence Lab</a><br/>
<a href="http://cs.stanford.edu" target='_blank'>Stanford University</a><br/>
</div>
<div>
    <b>Email:</b> <tt>kaichun [at] cs [dot] stanford [dot] edu</tt><br>
    <b>Office:</b> S297 James H. Clark Center<br>
</div>
<div>
<a href="resources/cv.pdf" target='_blank'>[CV (July, 2020)]</a>&nbsp;
<a href="#project">[Projects]</a>&nbsp;
<a href="#education">[Education]</a>&nbsp;
<a href="#experiences">[Experiences]</a>&nbsp;
<br><br>
<a href="https://scholar.google.com/citations?user=pL7JsOsAAAAJ&hl=en" target='_blank'>[Google Scholar]</a>&nbsp;
<a href="https://github.com/daerduocarey" target='_blank'>[GitHub]</a>
<a href="https://www.linkedin.com/in/kaichun-mo-a681b296/" target="_blank">[LinkedIn]</a>
<a href="https://twitter.com/KaichunMo" target="_blank">[Twitter]</a>
</div>
</td>
</tr>
</table>
<br>

<div class="w3-panel w3-leftbar w3-light-grey">
      <p class="w3-xlarge w3-serif">
      <i>"The biggest obstacle (in ML/AI development at the moment) is the difficulty for everyone to think differently and question conventional wisdom."</i></p>
        <p>--- Yann LeCun</p>
</div>


<div class="w3-panel w3-leftbar w3-light-grey">
      <p class="w3-xlarge w3-serif">
      <i>"The people who are crazy enough to think they can change the world are the ones who do."</i></p>
        <p>--- Steve Jobs</p>
</div>

<div class="w3-panel w3-leftbar w3-light-grey">
      <p class="w3-xlarge w3-serif">
      <i>白日不到处，青春恰自来。苔花如米小，亦学牡丹开。</i></p>
        <p>--- 《苔》【清】袁枚</p>
</div>


<h3>About</h3>
<div class="section">
<ul>
    <p>I am currently a fifth-year CS Ph.D. student advised by 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target='_blank'>Prof. Leonidas J. Guibas</a>. 

    <p>Prior to joining Stanford in 2016, I got my Bachelor degree from 
    <a href="http://acm.sjtu.edu.cn/" target="_blank">ACM Honored Class</a>, 
    <a href="http://zhiyuan.sjtu.edu.cn/" target="_blank">Zhiyuan College</a>, 
    <a href="http://www.sjtu.edu.cn/" target="_blank">Shanghai Jiao Tong University</a>.
    I got a GPA <b>3.96/4.30 (91.87/100)</b>, ranked <b>1/33</b> in the class.
    </p>

    <p>My research interests include 3D vision and graphics, deep learning, geometry processing, reinforcement learning and robotics.</p>

    <p><b>Collaborations:</b> Please feel free to contact me if you are interested in my research or potential collaborations.</p>

</ul>
</div>
<br>

<h3>News</h3>
<div class="section">
<ul>
<ul>
    <li><b style="color: green; background-color: #ffff42">NEW</b> [Apr, 2021] 
        Our ICCV2021 Workshop Call-for-paper: <a href="https://geometry.stanford.edu/struco3d/" target="_blank">StruCo3D2021: Structural and Compositional Learning on 3D Data</a>. Please consider submitting your works by July 26, 2021. See you online on Oct 11, 2021.</li>
    <li><b style="color: green; background-color: #ffff42">NEW</b> [Feb, 2021] DSG-Net gets provisional accept and then rejected (updated June 21, 2021) to ToG. However, here is the code: <a href="https://github.com/IGLICT/DSG-Net" target="_blank">[Github]</a>.</li>
    <li>[Sep, 2020] One paper gets accepted to NeurIPS 2020.</li>
    <li>[July, 2020] Two papers get accepted to ECCV 2020.</li>
    <li>[Feb, 2020] Two papers get accepted to CVPR 2020.</li>
    <li>[Dec, 2019] One paper gets accepted to ICLR 2020.</li>
    <li>[July, 2019] StructureNet gets accepted to Siggraph Asia 2019!</li>
    <li>[Feb, 2019] PartNet gets accepted to CVPR 2019!</li>
    <li>[February 2017] Our <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf" target="_blank">PointNet</a> is accepted as an oral presentation in CVPR 2017, Honolulu, USA.</li>
</ul>
</div>
<br>

<a name="project"></a>
<h3>Projects</h3>
<div class="mainsection">
<ul>

<table width="100%">

    
<!-- Where2Act -->
<tr>
<td width="30%" valign="top"><p><img src="papers/where2act.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Where2Act: From Pixels to Actions for Articulated 3D Objects</h3>
    <strong>Kaichun Mo</strong>, 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a>,
    <a href="http://www.mustafamukadam.com/" target='_blank'>Mustafa Mukadam</a>,
    <a href="https://www.cs.cmu.edu/~abhinavg/" target='_blank'>Abhinav Gupta</a> and
    <a href="https://shubhtuls.github.io/" target='_blank'>Shubham Tulsiani</a><br>
    arXiv:2101.02692 [cs.CV]<br>
    <p>
    One of the fundamental goals of visual perception is to allow agents to meaningfully interact with their environment. In this paper, we take a step towards that long-term goal -- we extract highly localized actionable information related to elementary actions such as pushing or pulling for articulated objects with movable parts. For example, given a drawer, our network predicts that applying a pulling force on the handle opens the drawer. We propose, discuss, and evaluate novel network architectures that given image and depth data, predict the set of actions possible at each pixel, and the regions over articulated parts that are likely to move under the force. We propose a learning-from-interaction framework with an online data sampling strategy that allows us to train the network in simulation (SAPIEN) and generalizes across categories. But more importantly, our learned models even transfer to real-world data.
    </p>
    <a href="https://arxiv.org/abs/2101.02692" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/where2act" target="_blank">[Project]</a>
    <a href="where2act/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- GenStruct -->
<tr>
<td width="30%" valign="top"><p><img src="papers/genstruct.gif" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Compositionally Generalizable 3D Structure Prediction</h3>
    <a href="http://hansf.me/" target="_blank">Songfang Han</a>,
    <a href="http://cseweb.ucsd.edu/~jigu/" target="_blank">Jiayuan Gu</a>, 
    <strong>Kaichun Mo</strong>, 
    <a href="https://ericyi.github.io/" target="_blank">Li Yi</a>, 
    <a href="https://samhu1989.github.io/" target="_blank">Siyu Hu</a>,
    <a href="http://staff.ustc.edu.cn/~xjchen99/" target="_blank">Xuejin Chen</a> and
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><br>
    arXiv:2012.02493 [cs.CV]<br>
    <p>
    We tackle the task of 3D shape structure reconstruction from single images in a zero-shot setting, where we train our model on chairs only and show its generalizability to unseen novel categories (e.g. tables, cabinets) without any new training data.
    We bring in the concept of compositional generalizability and propose a novel framework that factorizes the 3D shape reconstruction problem into proper sub-problems, each of which is tackled by a carefully designed neural sub-module with generalizability guarantee. 
    The intuition behind our formulation is that object parts (slates and cylindrical parts), their relationships (adjacency, equal-length, and parallelism) and shape substructures (T-junctions and a symmetric group of parts) are mostly shared across object categories, even though the object geometry may look very different. 
    </p>
    <a href="https://arxiv.org/abs/2012.02493" target="_blank">[Paper]</a>
    <a href="genstruct/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- DSG-Net -->
<tr>
<td width="30%" valign="top"><p><img src="papers/dsmnet.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape Generation</h3>
    <a href="http://people.geometrylearning.com/~jieyang/" target="_blank">Jie Yang</a><sup>*</sup>,
    <strong>Kaichun Mo</strong><sup>*</sup>, 
    <a href="http://users.cs.cf.ac.uk/Yukun.Lai/" target="_blank">Yu-Kun Lai</a>,
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a> and
    <a href="http://geometrylearning.com/lin/" target="_blank">Lin Gao</a><br>
    arXiv:2008.05440 [cs.GR]<br>
    <p>
    We introduce DSG-Net, a deep neural network that learns a disentangled structured mesh representation for 3D shapes, where two key aspects of shapes, geometry and structure, are encoded in a synergistic manner to ensure plausibility of the generated shapes, while also being disentangled as much as possible. This supports a range of novel shape generation applications with intuitive control, such as interpolation of structure (geometry) while keeping geometry (structure) unchanged.
    Our method not only supports controllable generation applications, but also produces high-quality synthesized shapes, outperforming state-of-the-art methods.
    </p>
    <a href="https://arxiv.org/abs/2008.05440" target="_blank">[Paper]</a>
    <a href="http://geometrylearning.com/dsg-net/" target="_blank">[Project]</a>
    <a href="https://youtu.be/UJH0dITlfsw" target="_blank">[Video]</a>
    <a href="https://cs.stanford.edu/~kaichun/dsgnet/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>



<!-- GenPartAss -->
<tr>
<td width="30%" valign="top"><p><img src="papers/genpartass.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Generative 3D Part Assembly via Dynamic Graph Learning</h3>
    <a href="https://jialeihuang.github.io/" target="_blank">Jialei Huang</a><sup>*</sup>, 
    <a href="https://championchess.github.io/" target="_blank">Guanqi Zhan</a><sup>*</sup>, 
    <a href="https://fqnchina.github.io/" target="_blank">Qingnan Fan</a>, 
    <strong>Kaichun Mo</strong>, 
    <a href="https://linsats.github.io/" target="_blank">Lin Shao</a>,
    <a href="https://cfcs.pku.edu.cn/baoquan/" target="_blank">Baoquan Chen</a>,
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a> and
    <a href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><br>
    <a href="https://nips.cc/Conferences/2020/" target="_blank">NeurIPS 2020</a><br>
    <p>
    Analogous to buying an IKEA furniture, given a set of 3D part point clouds, we predict 6-Dof part poses to assemble a 3D shape.
    To tackle this problem, we propose an assembly-oriented dynamic graph learning framework that leverages an iterative graph neural network as a backbone. 
    It explicitly conducts sequential part assembly refinements in a coarse-to-fine manner, exploits a pair of part relation reasoning module and part aggregation module for dynamically adjusting both part features and their relations in the part graph.
    </p>
    <a href="https://arxiv.org/abs/2006.07793" target="_blank">[Paper]</a>
    <a href="https://hyperplane-lab.github.io/Generative-3D-Part-Assembly/" target="_blank">[Project]</a>
    <a href="genpartass/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>



    
<!-- Rethink PC-GAN Sampling -->
<tr>
<td width="30%" valign="top"><p><img src="papers/rethinkpcgan.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks</h3>
    <a href="http://ai.stanford.edu/~hewang/" target="_blank">He Wang</a><sup>*</sup>, 
    <a href="https://github.com/JzMaple" target="_blank">Zetian Jiang</a><sup>*</sup>, 
    <a href="https://cs.stanford.edu/~ericyi/" target="_blank">Li Yi</a>, 
    <strong>Kaichun Mo</strong>, 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    arXiv:2006.07029 [cs.CV]<br>
    <p>
    We show that sampling-insensitive discriminators (e.g.PointNet-Max) produce shape point clouds with point clustering artifacts while sampling-oversensitive discriminators (e.g.PointNet++, DGCNN) fail to guide valid shape generation. 
    We propose the concept of sampling spectrum to depict the different sampling sensitivities of discriminators. 
    We point out that, though recent research has been focused on the generator design, the main bottleneck of point cloud GAN actually lies in the discriminator design.
    </p>
    <a href="https://arxiv.org/abs/2006.07029" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/rethinkpcgan/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

   
<!-- ImPartAss -->
<tr>
<td width="30%" valign="top"><p><img src="papers/impartass.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Learning 3D Part Assembly from a Single Image</h3>
    <a href="https://antheali.github.io/" target="_blank">Yichen Li</a><sup>*</sup>,
    <strong>Kaichun Mo</strong><sup>*</sup>, 
    <a href="https://linsats.github.io/" target="_blank">Lin Shao</a>,
    <a href="https://mhsung.github.io/" target="_blank">Minhyuk Sung</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://eccv2020.eu/" target="_blank">ECCV 2020</a><br>
    <i>(Also be present at <a href="https://holistic-3d.github.io/eccv20/" target="_blank">Holistic Scene Structures for 3D Vision</a>)</i><br>
    <p>
    We introduce a novel problem, single-image-guided 3D part assembly, that assembles 3D shapes from parts given a complete set of part point cloud scans and a single 2D image depicting the object.
    The task is motivated by the robotic assembly setting and the estimated per-part poses serve as a vision-based initialization before robotic planning and control components.
    </p>
    <a href="https://arxiv.org/abs/2003.09754" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~liyichen/projects/assembly/" target="_blank">[Project]</a>
    <a href="https://youtu.be/TCME39wusek" target="_blank">[Video (1-min)]</a>
    <a href="https://youtu.be/gtaBaEAs22s" target="_blank">[Video (7-min)]</a>
    <a href="https://cs.stanford.edu/~kaichun/impartass/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

    
<!-- PT2PC -->
<tr>
<td width="30%" valign="top"><p><img src="papers/pt2pc.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions</h3>
    <strong>Kaichun Mo</strong>, 
    <a href="http://ai.stanford.edu/~hewang/" target="_blank">He Wang</a>,
    <a href="https://sites.google.com/site/skywalkeryxc/" target="_blank">Xinchen Yan</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://eccv2020.eu/" target="_blank">ECCV 2020</a>
    <p>
    This paper investigates the novel problem of generating 3D shape point cloud geometry from a symbolic part tree representation. 
    In order to learn such a conditional shape generation procedure in an end-to-end fashion, we propose a conditional GAN "part tree"-to-"point cloud" model (PT2PC) that disentangles the structural and geometric factors. 
    </p>
    <a href="https://arxiv.org/abs/2003.08624" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/pt2pc/" target="_blank">[Project]</a>
    <a href="https://youtu.be/GZGxaFx-kgw" target="_blank">[Video (1-min)]</a>
    <a href="https://youtu.be/LYVdoTWfy5I" target="_blank">[Video (10-min)]</a>
    <a href="https://cs.stanford.edu/~kaichun/pt2pc/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

<!-- SAPIEN -->
<tr>
<td width="30%" valign="top"><p><img src="papers/sapien.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>SAPIEN: A SimulAted Part-based Interactive ENvironment</h3>
    <a href="https://www.fbxiang.com/" target="_blank">Fanbo Xiang</a>, 
    <a href="https://yzqin.github.io/" target="_blank">Yuzhe Qin</a>, 
    <strong>Kaichun Mo</strong>, 
    <a href="https://www.linkedin.com/in/yikuan-xia-4418a9170/" target="_blank">Yikuan Xia</a>, 
    <a href="https://berniezhu.github.io/" target="_blank">Hao Zhu</a>, 
    <a href="https://fangchenliu.github.io/" target="_blank">Fangchen Liu</a>, 
    <a href="http://cseweb.ucsd.edu/~mil070/" target="_blank">Minghua Liu</a>, 
    <a href="https://jianghanxiao.github.io/" target="_blank">Hanxiao Jiang</a>, 
    Yifu Yuan</a>, 
    <a href="http://ai.stanford.edu/~hewang/" target="_blank">He Wang</a>, 
    <a href="https://cs.stanford.edu/~ericyi/" target="_blank">Li Yi</a>, 
    <a href="https://angelxuanchang.github.io/" target="_blank">Angel X.Chang</a>, 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a> and
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><br>
    <a href="http://cvpr2020.thecvf.com/" target="_blank">CVPR 2020</a>, <i style="color: red">Oral Presentation</i>
    <p>
    We propose a realistic and physics-rich simulation environment hosting large-scale 3D articulated objects from ShapeNet and PartNet. 
    Our PartNet-Mobility dataset contains 14,068 articulated parts with part motion information for 2,346 object models from 46 common indoor object categories.
    SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.
    </p>
    <a href="https://arxiv.org/abs/2003.08515" target="_blank">[Paper]</a>
    <a href="http://sapien.ucsd.edu/publication" target="_blank">[Project Page]</a>
    <a href="https://github.com/haosulab/SAPIEN-Release" target="_blank">[Code]</a>
    <a href="https://youtu.be/K2yOeJhJXzM" target="_blank">[Demo]</a>
    <a href="https://youtu.be/WnTxEeSHebk" target="_blank">[Video]</a>
    <a href="https://cs.stanford.edu/~kaichun/sapien/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>
    
<!-- StructEdit -->
<tr>
<td width="30%" valign="top"><p><img src="papers/structedit.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>StructEdit: Learning Structural Shape Variations</h3>
    <strong>Kaichun Mo</strong><sup>*</sup>, 
    <a href="http://paulguerrero.net/" target="_blank">Paul Guerrero</a><sup>*</sup>, 
    <a href="https://cs.stanford.edu/~ericyi/" target="_blank">Li Yi</a>, 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a>, 
    <a href="http://peterwonka.net/" target="_blank">Peter Wonka</a>, 
    <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/" target="_blank">Niloy Mitra</a> and 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="http://cvpr2020.thecvf.com/" target="_blank">CVPR 2020</a><br>
    <i><b style="color: red;">Featured in: </b>
        <a href="https://www.rsipvision.com/CVPR2020-Tuesday/26/" target="_blank">CVPR Daily (Tue)</a>
    </i><br>
    <i><b style="color: red;">Video: </b>
        <a href="https://www.youtube.com/watch?v=Er2sA-lfsI8" target="_blank">CVPR Workshop: Learning 3D Generative Models (Invited Talk By Paul Guerrero)</a>
    </i><br>
    <p>
    We learn local shape edits (shape deltas) space that captures both discrete structural changes and continuous variations.
    Our approach is based on a conditional variational autoencoder (cVAE) for encoding and decoding shape deltas, conditioned on a source shape.
    The learned shape delta spaces support shape edit suggestions, shape analogy, and shape edit transfer, much better than StructureNet, on the PartNet dataset.
    </p>
    <a href="https://arxiv.org/abs/1911.11098" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/structedit/" target="_blank">[Project]</a>
    <a href="https://youtu.be/OrUzdkRUoE0" target="_blank">[Video]</a>
    <a href="https://cs.stanford.edu/~kaichun/structedit/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

<!-- LearningToGroup -->
<tr>
<td width="30%" valign="top"><p><img src="papers/learningtogroup.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories</h3>
    <a href="https://tiangeluo.github.io/" target="_blank">Tiange Luo</a>, 
    <strong>Kaichun Mo</strong>, 
    <a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>, 
    <a href="http://jerryxu.net/" target="_blank">Jiarui Xu</a>,
    <a href="https://samhu1989.github.io/" target="_blank">Siyu Hu</a>, 
    <a href="https://scholar.google.com/citations?user=VZHxoh8AAAAJ" target="_blank">Liwei Wang</a> and 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><br>
    <a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a><br>
    <p>
    We address the problem of learning to discover 3D parts for objects in unseen categories under the zero-shot learning setting.
    We propose a learning-based iterative grouping framework which learns a grouping policy to progressively merge small part proposals into bigger ones in a bottom-up fashion.
    On PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen categories.
    </p>
    <a href="https://arxiv.org/abs/2002.06478" target="_blank">[Paper]</a>
    <a href="https://tiangeluo.github.io/projectpages/ltg.html" target="_blank">[Project]</a>
    <a href="https://iclr.cc/virtual_2020/poster_rkl8dlHYvB.html" target="_blank">[Video]</a>
    <a href="https://cs.stanford.edu/~kaichun/learningtogroup/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

<!-- StructureNet -->
<tr>
<td width="30%" valign="top"><p><img src="papers/structurenet.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>StructureNet: Hierarchical Graph Networks for 3D Shape Generation</h3>
    <strong>Kaichun Mo</strong><sup>*</sup>, 
    <a href="http://paulguerrero.net/" target="_blank">Paul Guerrero</a><sup>*</sup>, 
    <a href="https://cs.stanford.edu/~ericyi/" target="_blank">Li Yi</a>, 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a>, 
    <a href="http://peterwonka.net/" target="_blank">Peter Wonka</a>, 
    <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/" target="_blank">Niloy Mitra</a> and 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://sa2019.siggraph.org/" target="_blank">ACM Transactions on Graphics (SIGGRAPH Asia 2019)</a><br>
    <p>
    We introduce a hierarchical graph network for learning structure-aware shape generation which (i) can directly encode shape parts represented as such n-ary graphs; (ii) can be robustly trained on large and complex shape families such as PartNet; and (iii) can be used to generate a great diversity of realistic structured shape geometries with both both continuous geometric and discrete structural variations.
    </p>
    <a href="https://arxiv.org/abs/1908.00575" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/structurenet/" target="_blank">[Project]</a>
    <a href="https://cs.stanford.edu/~kaichun/structurenet/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- PartNet -->
<tr>
<td width="30%" valign="top"><p><img src="papers/partnet.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</h3>
    <strong>Kaichun Mo</strong>, 
    <a href="http://cseweb.ucsd.edu/~shz338/" target="_blank">Shilin Zhu</a>, 
    <a href="https://angelxuanchang.github.io/" target="_blank">Angel X.Chang</a>, 
    <a href="https://cs.stanford.edu/~ericyi/" target="_blank">Li Yi</a>, 
    <a href="https://subarnatripathi.github.io/" target="_blank">Subarna Tripathi</a>, 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a> and 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><br>
    <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a><br>
    <i>(Also be present at <a href="https://sites.google.com/view/fgvc6/home" target="_blank">The Sixth Workshop on Fine-Grained Visual Categorization</a> and <a href="https://3dscenegen.github.io/" target="_blank">the 3D Scene Generation Workshop</a>)</i><br>
    <i><b style="color: red;">Featured in: </b>
        <a href="https://www.therobotreport.com/intel-osu-uc-san-diego-reinforcement-learning-partnet-robots/" target="_blank">The Robot Report</a>,
        <a href="https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/partnet-helps-robots-understand-what-things-are" target="_blank">IEEE Spectrum</a>,
        <a href="https://www.roboticsbusinessreview.com/ai/researchers-launch-26k-object-dataset-to-help-robots-learn-shapes/" target="_blank">Robotics Business Review</a>, 
        <a href="https://techcrunch.com/2019/06/17/intel-is-doing-the-hard-work-necessary-to-make-sure-robots-can-operate-your-microwave/" target="_blank">TechCrunch</a>, 
        <a href="https://venturebeat.com/2019/06/17/intel-highlights-ai-that-can-see-around-corners-coach-children-on-the-autism-spectrum-and-more-during-cvpr/" target="_blank">VentureBeat</a>, 
        <a href="https://www.intel.ai/introducing-partnet/" target="_blank">Intel AI Blog</a>, 
        <a href="https://newsroom.intel.com/news/intel-offers-computer-powered-echolocation-tech-artificial-intelligence-research-cvpr-2019" target="_blank">Intel Newsroom</a>
    </i><br>
    <p>
    We propose a 3D object database with fine-grained and hierarchical part annotation, to assist segmentation and affordance research. We benchmark three part-level object understanding tasks: fine-grained semantic segmentation, hierarchical semantic segmentation and instance segmentation. We also propose a novel method for instance segmentation.
    </p>
    <a href="https://arxiv.org/abs/1812.02713" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/partnet/" target="_blank">[Project]</a>
    <a href="https://cs.stanford.edu/~kaichun/partnet/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

<!-- AdobeIndoorNav -->
<tr>
<td width="30%" valign="top"><p><img src="papers/adobeindoornav.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>The AdobeIndoorNav Dataset: Towards Deep Reinforcement Learning based Real-world Indoor Robot Visual Navigation</h3>
    <strong>Kaichun Mo</strong>, 
    <a href="http://haoxiang.org/academic-homepage/" target="_blank">Haoxiang Li</a>, 
    <a href="https://research.adobe.com/person/zhe-lin/" target="_blank">Zhe Lin</a> and 
    <a href="https://joonyoung-cv.github.io/" target="_blank">Joon-Young Lee</a><br>
    arXiv:1802.08824 [cs.RO]<br>
    <p>
    We propose an indoor navigation dataset for visual navigation and deep reinforcement learning research. We show that the current mapless DRL-based method suffers from the target generalization and scene generalization issues. We propose methods to improve target generalization.
    </p>
    <a href="https://arxiv.org/abs/1802.08824" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/adobeindoornav/" target="_blank">[Project]</a>
    <a href="https://cs.stanford.edu/~kaichun/adobeindoornav/paper.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

<!-- PointNet -->
<tr>
<td width="30%" valign="top"><p><img src="papers/pointnet.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</h3>
    <a href="https://web.stanford.edu/~rqi/" target="_blank">Charles R. Qi</a><sup>*</sup>, 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><sup>*</sup>, 
    <strong>Kaichun Mo</strong> and 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="http://cvpr2017.thecvf.com/" target="_blank">CVPR 2017</a>, <i style="color: red">Oral Presentation</i><br>
    <p>
    We propose novel neural networks to directly consume an unordered point cloud as input, without converting to other 3D representations such as voxel grids first. Rich theoretical and empirical analyses are provided.
    </p>
    <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf" target="_blank">[Paper]</a>
    <a href="http://stanford.edu/~rqi/pointnet/" target="_blank">[Project]</a>
    <a href="https://cs.stanford.edu/~kaichun/papers/pointnet.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

<!-- AdobeIndoorNav -->
<tr>
<td width="30%" valign="top"><p><img src="papers/kaczmarz.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Accelerating Random Kaczmarz Algorithm Based on Clustering Information</h3>
    <a href="http://bcmi.sjtu.edu.cn/~liyujun/" target="_blank">Yujun Li</a>, 
    <strong>Kaichun Mo</strong> and 
    <a href="https://dblp.org/pers/hd/y/Ye:Haishan" target="_blank">Haishan Ye</a><br>
    <a href="https://www.aaai.org/Conferences/AAAI/aaai16.php" target="_blank">AAAI 2016</a><br>
    <p>
    Using the property of randomly sampled data in high-dimensional space, we propose an accelerated algorithm based on clustering information to improve block Kaczmarz and Kaczmarz via Johnson-Lindenstrauss lemma. Additionally, we theoretically demonstrate convergence improvement on block Kaczmarz algorithm.
    </p>
    <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11938" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/papers/kaczmarz.bib" target="_blank">[Bibtex]</a>
</p></td>
</tr>

<tr><td>(<sup>*</sup>: indicates equal contribution.)</td></tr>

</table>
</ul>
</div>
<br>

<a name="talks"></a>
<h3>Invited Talks</h3>
<div class="section">
<ul>
    <li>[June 2021] Learning 3D Shape Structure and Semantics, at <a href="https://jiajunwu.com/" target="_blank">CogAI Reading Group</a>, Stanford.</li>
    <li>[April 2021] Learning 3D Shape Actionable Information from Simulated Interaction, at <a href="http://www.sfu.ca/visual-computing" target="_blank">SFU VCR (visual computing and robotics) seminar</a>.</li>
    <li>[March 2021] Learning 3D Shape Actionable Information from Simulated Interaction, at <a href="https://www.autodesk.com/research" target="_blank">Autodesk Research</a>.</li>
    <li>[Feb 2021] Where2Act: From Pixels to Actions for Articulated 3D Objects, for <a href="https://www.imperial.ac.uk/matchlab/" target="_blank">the MatchLab</a> at Imperial College London.</li>
    <li>[Feb 2021] Where2Act: From Pixels to Actions for Articulated 3D Objects, for <a href="https://prior.allenai.org/" target="_blank">the PRIOR team</a> at AI2.</li>
    <li>[June 2020] Part-level and Structural 3D Shape Understanding, at <a href="http://geometrylearning.com/" target="_blank">Intelligent Graphics Laboratory (IGL)</a>.</li>
    <li>[June 2020] <a href="https://www.bilibili.com/video/BV1e7411c7kR?p=42" target="_blank">Part-level and Structural Understanding for 3D shape Perception, Synthesis and Editing (in Chinese)</a>, <a href="http://games-cn.org" target="_blank">GAMES: Graphics And Mixed Environment Seminar</a>.</li>
    <li>[April 2019] PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding, at Stanford GCafe Seminar.</li>
</ul>
</div>
<br>

<a name="education"></a>
<h3>Education</h3>
<div class="mainsection">
<ul>
<table width="100%">

<!-- Stanford -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/stanford.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>CS Ph.D. Candidate, Geometric Computation Group and Artificial Intelligence Lab, Stanford University</b><br/>2016.9 - Present<br/><br/>
        Advisor:
            <a href="http://geometry.stanford.edu/member/guibas/index.html" target='_blank'>Prof. Leonidas J. Guibas</a>
        </p></td>
</tr>
<tr></tr>

<!-- UCSD -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/ucsd.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Visiting Graduate, Su Lab, University of California, San Diego</b><br/>2019.7 - 9<br/><br/>
        Advisor:
            <a href="http://cseweb.ucsd.edu/~haosu/" target='_blank'>Prof. Hao Su</a>
        </p></td>
</tr>
<tr></tr>

<!-- Cornell -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/cornell.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Exchange Research Student, Graphics and Vision Lab, Cornell University</b><br/>2015.6 - 2015.12<br/><br/>
        Research Advisor: <a href="http://www.cs.cornell.edu/~kb/" target="_blank">Prof. Kavita Bala</a><br/>
        Academic Advisor: <a href="http://www.cs.cornell.edu/jeh/" target="_blank">Prof. John E. Hopcroft</a><br/>
        </p></td>
</tr>
<tr></tr>

<!-- SJTU -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/sjtu.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Bachelor of Engineering in Science, ACM Honored Class, Zhiyuan College, Shanghai Jiao Tong University</b><br/>2012.9 - 2016.6<br/><br/>
        Research Advisor: <a href="http://www.math.pku.edu.cn/teachers/zhzhang/" target="_blank">Prof. Zhihua Zhang</a><br>
        Academic Advisor: <a href="http://apex.sjtu.edu.cn/members/yyu" target="_blank">Prof. Yong Yu</a><br/>
        </p></td>
</tr>
<tr></tr>

</table>
</ul>
</div>
<br>

<a name="experiences"></a>
<h3>Working Experiences</h3>
<div class="mainsection">
<ul>
<table width="100%">

<!-- Facebook intern-->
<tr>
    <td width="15%" valign="top"><p><img src="logos/facebook.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Research Intern, Facebook AI Research (<strike>Pittsburgh</strike> Remote from California)</b><br/>2020.6 - 2020.9<br/><br/>
        Mentor:
            <a href="https://shubhtuls.github.io/" target='_blank'>Shubham Tulsiani</a>,
            <a href="http://www.mustafamukadam.com/" target='_blank'>Mustafa Mukadam</a> and 
            <a href="https://www.cs.cmu.edu/~abhinavg/" target='_blank'>Prof. Abhinav Gupta</a>
        </p></td>
</tr>
<tr></tr>

<!-- Autodesk intern-->
<tr>
    <td width="15%" valign="top"><p><img src="logos/autodesk.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Research Intern, AI Lab, Autodesk Research</b><br/>2018.6 - 2018.9<br/><br/>
        Mentor:
            <a href="https://autodeskresearch.com/people/mike-haley" target="_blank">Mike Haley</a>
        </p></td>
</tr>
<tr></tr>

<!-- Adobe intern-->
<tr>
    <td width="15%" valign="top"><p><img src="logos/adobe.jpg" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Research Intern, Creative Intelligence Lab, Adobe Research</b><br/>2017.6 - 2017.9<br/><br/>
        Mentors: 
            <a href="http://haoxiang.org/academic-homepage/" target="_blank">Haoxiang Li</a>,
            <a href="https://joonyoung-cv.github.io/" target="_blank">Joon-Young Lee</a>,
            <a href="https://research.adobe.com/person/zhe-lin/" target="_blank">Zhe Lin</a> and 
            <a href="http://www.meyumer.com/" target="_blank">Ersin Yumer</a>
        </p></td>
</tr>

</table>
</div>
</ul>
<br>

<a name="teaching"></a>
<h3>Teaching Experiences</h3>
<div class="section">
<ul>
    <li>Guest Lecturer, Spring 2021, <a href="http://graphics.stanford.edu/courses/cs233-21-spring/" target="_blank">Geometric and Topological Data Analysis (CS 233)</a>, Stanford University</li>
    <li>Guest Lecturer, Winter 2021, <a href="https://haosulab.github.io/ml-meets-geometry/WI21/index.html" target="_blank">Machine Learning Meets Geometry (CSE 291-I00)</a>, UCSD</li>
    <li>Teaching Assistant, Spring 2020, <a href="http://graphics.stanford.edu/courses/cs233-20-spring/" target="_blank">Geometric and Topological Data Analysis (CS 233)</a>, Stanford University</li>
    <li>Guest Lecturer, Spring 2018, <a href="http://graphics.stanford.edu/courses/cs233-18-spring/" target="_blank">Geometric and Topological Data Analysis (CS 233)</a>, Stanford University</li>
    <li>Teaching Assistant, Fall 2014, Introduction To Computer Science (CS 120), Shanghai Jiao Tong University</li>
</ul>
</div>
<br>

<a name="reviewer"></a>
<h3>Professional Activities</h3>
<div class="section">
<ul>
    <li>Workshop Organizer: 
        <a href="https://geometry.stanford.edu/struco3d/" target="_blank">StruCo3D2021: Structural and Compositional Learning on 3D Data (ICCV 2021)</a>
    </li>
    <li>Conference Reviewer: ICML 2021; ICLR 2021, 2022; NeurIPS 2020, 2021; NeurIPS Datasets and Benchmarks Track 2021; CVPR 2020, 2021 (outstanding reviewer); ICCV 2019, 2021; ECCV 2020; RSS 2021; ICRA 2020; IROS 2021; Siggraph 2021; Siggraph Asia 2020, 2021; AAAI 2020, 2021; ACCV 2020; 3DV 2017, 2018, 2019, 2020, 2021; Pacific Graphics 2020; WACV 2020, 2021, 2022; MVA 2019;</li>
    <li>Workshop Reviewer: CICV (Compositionality in Computer Vision) 2020, 3DRW (3D Reconstruction in the Wild) 2018, 2019; VLEASE (Visual Learning and Embodied Agents in Simulation Environments) 2018;</li>
    <li>Journal Reviewer: Robotics and Automation Letters (RA-L); Transactions on Pattern Analysis and Machine Intelligence (TPAMI); IEEE Transactions on Visualization and Computer Graphics (TVCG); IEEE Transactions on Image Processing (TIP); IEEE Transactions on Multimedia; IEEE Transactions on Robotics (TRO); ACM Transactions on Graphics (TOG); Signal Processing: Image Communication; Computers &amp; Graphics; Information Fusion; International Journal of Advanced Robotic Systems.</li>
</ul>
</div>
<br>

<a name="honor"></a>
<h3>Honors and Awards</h3>
<div class="section">
<ul>
    <li>[2016-2017] School of Engineering Fellowship, Stanford University</li>
    <li>[Spring 2015] Meritorious Winner, 2015 Mathematical Contest In Modeling(MCM), <a href="resources/2015MCM_paper.pdf" target="_blank">[Paper]</a></li>
    <li>[Fall 2015], National Scholarship, Shanghai Jiao Tong University</li>
    <li>[Fall 2014], National Scholarship, Shanghai Jiao Tong University</li>
    <li>[Fall 2013], KoGuan Scholarship, Shanghai Jiao Tong University</li>
    <li>[Fall 2011], The First Prize, National High School Mathematics Contest, Tianjin, China</li>
    <li>[Fall 2010], The First Prize, National Olympiad in Informatics in Provinces, Tianjin, China</li>
</ul>
</div>
<br>


<hr/>
<div id="footer" style="font-size:10">Kaichun Mo <i>Last updated: Augest, 2020</i></div>
</body>
